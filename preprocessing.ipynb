{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from utility_functions import load_dataset\n",
    "\n",
    "path_to_file = '/home/alexandr/data/audio/'\n",
    "path_to_file_test = '/home/alexandr/data/test/'\n",
    "path_to_file_unknown = '/home/alexandr/data/unknown/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract classes from txt file\n",
    "\n",
    "raw_classes = np.loadtxt('/home/alexandr/data/meta/meta.txt', dtype='str')\n",
    "#print(raw_classes)\n",
    "classes = np.delete(raw_classes, [1, 2, 3], 1)\n",
    "text_labels = ['background', 'bags', 'door', 'keyboard', 'knocking_door', 'ring', 'speech', 'tool']\n",
    "\n",
    "for index, label in enumerate(text_labels):\n",
    "    classes[classes == label] = index\n",
    "\n",
    "dict_classes = dict(classes)\n",
    "classes_test = np.loadtxt('./meta-test.txt',dtype='str')\n",
    "dict_classes_test = dict(classes_test)\n",
    "\n",
    "raw_data_train = np.array(load_dataset(path_to_file, dict_classes))\n",
    "raw_data_test = np.array(load_dataset(path_to_file_test, dict_classes_test))\n",
    "raw_data_unknown = np.array(load_dataset(path_to_file_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detector import detect_startpoint\n",
    "# \n",
    "# cropped_data_train = []\n",
    "# for sound in raw_data_train[:,0]:\n",
    "#     start_point = detect_startpoint(sound)\n",
    "#     cropped_data_train.append(sound[start_point:])\n",
    "# cropped_data_train = np.array(cropped_data_train)\n",
    "# print (1)\n",
    "# cropped_data_test = []\n",
    "# for sound in raw_data_test[:,0]:\n",
    "#     start_point = detect_startpoint(sound)\n",
    "#     cropped_data_test.append(sound[start_point:])\n",
    "# cropped_data_test = np.array(cropped_data_test)\n",
    "cropped_data_train =raw_data_train\n",
    "cropped_data_test = raw_data_test\n",
    "cropped_data_unknown = raw_data_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first second segmentation\n",
    "from utility_functions import find_longest, padding_data, get_random_segment\n",
    "\n",
    "# max_length = find_longest(raw_data)\n",
    "# pad_data = padding_data(raw_data, max_length)\n",
    "segmented_data_train = []\n",
    "\n",
    "for index, sound in enumerate(cropped_data_train[:,0]):\n",
    "    segmented_data_train.append(get_random_segment(sound))\n",
    "    \n",
    "segmented_data_train = np.array(segmented_data_train)\n",
    "\n",
    "\n",
    "segmented_data_test= []\n",
    "\n",
    "for index, sound in enumerate(cropped_data_test[:,0]):\n",
    "    segmented_data_test.append(get_random_segment(sound))\n",
    "    \n",
    "segmented_data_test = np.array(segmented_data_test)\n",
    "\n",
    "segmented_data_unknown = []\n",
    "\n",
    "for index, sound in enumerate(cropped_data_unknown):\n",
    "    segmented_data_unknown.append(get_random_segment(sound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean memory\n",
    "pad_data = None\n",
    "raw_data = None\n",
    "extracted_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11307, 48000)\n(473, 48000)\n(11307, 2)\n(473, 2)\n(137, 48000)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(segmented_data_train).shape)\n",
    "print(np.array(segmented_data_test).shape)\n",
    "print(np.array(raw_data_train).shape)\n",
    "print(np.array(raw_data_test).shape)\n",
    "print(np.array(segmented_data_unknown).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from speechpy.feature import lmfe\n",
    "\n",
    "extracted_features_train = []\n",
    "extracted_features_test = []\n",
    "extracted_features_unknown = []\n",
    "\n",
    "def extract_feature(mode='mfcc'):\n",
    "    if mode == 'mfcc':\n",
    "        for sound in segmented_data_train:\n",
    "            feature = librosa.feature.mfcc(sound.astype(float), sr=16000, n_mfcc=20)\n",
    "            extracted_features_train.append(feature)\n",
    "\n",
    "        for sound in segmented_data_test:\n",
    "            extracted_features_test.append(librosa.feature.mfcc(sound.astype(float), sr=16000, n_mfcc=20))\n",
    "    elif mode == 'mbe':\n",
    "\n",
    "        for sound in segmented_data_train:\n",
    "            feature = lmfe(sound.astype(float), 16000)\n",
    "            extracted_features_train.append(feature)\n",
    "\n",
    "        for sound in segmented_data_test:\n",
    "            extracted_features_test.append(lmfe(sound.astype(float), 16000))\n",
    "            \n",
    "        for sound in segmented_data_unknown:\n",
    "            extracted_features_unknown.append(lmfe(sound.astype(float), 16000))    \n",
    "            \n",
    "extract_feature('mbe')\n",
    "x_train = np.array(extracted_features_train)\n",
    "x_test = np.array(extracted_features_test)\n",
    "x_unknown = np.array(extracted_features_unknown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11307, 298, 40)\n(11307,)\n(473, 298, 40)\n(473,)\n(137, 298, 40)\n"
     ]
    }
   ],
   "source": [
    "# temporary cell\n",
    "print(x_train.shape)\n",
    "y_train = np.array(raw_data_train[:,1])\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(raw_data_test[:,1])\n",
    "print(y_test.shape)\n",
    "\n",
    "print(x_unknown.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_test[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mbe_3sec_random_default'\n",
    "np.save('/home/alexandr/'+ filename, x_train)\n",
    "np.save('/home/alexandr/y', y_train)\n",
    "np.save('/home/alexandr/' + filename+ '_test', x_test)\n",
    "np.save('/home/alexandr/y_test', y_test)\n",
    "np.save('/home/alexandr/x_unknown',x_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(raw_data_test[:,1])\n",
    "\n",
    "np.save('/home/alexandr/y_test', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
